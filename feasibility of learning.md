<!-- Notes Day 2 -->
<p>
<b>Feasibility of Learning</b>
Learning proÂ­duces a hypothesis g to approximate the unknown target function f. If learning is successful, then g should approximate f well, which means Eout(g) ~ 0. However, this is not what we get from the probabilistic analysis. What we get instead is Eout(g) ~ Ein(g). We still have to make Ein(g) ~ 0 in order to conclude that Eout(g)~ 0.
<br />
We cannot guarantee that we will find a hypothesis that achieves Ein(g) ~ 0 but at least we will know if we find it. Remember that Eout (g) is an unknown quantity, since f is unknown, but Ein(g) is a quantity that we can evaluate. We have thus traded the condition Eout (g) ~ 0, one that we cannot ascertain, for the condition Ein(g) ~ 0, which we can ascertain. 
<br />
The feasibility of learning is thus split into two questions:<br/>
1. Can we make sure that Eout(g) is close enough to Ein(g)? <br/>
2. Can we make Ein(g) small enough?
</p>

<!-- End Notes Day 2 -->
